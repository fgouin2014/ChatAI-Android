# Guide Complet : KITT + Ollama + Oobabooga

Vous avez choisi les options **#3**, **#5** et **#7** :
- **#3** : Ollama Cloud (modÃ¨les puissants hÃ©bergÃ©s)
- **#5** : Ollama Local (modÃ¨les sur votre PC)
- **#7** : Oobabooga Text Gen WebUI (maximum de contrÃ´le)

**Bonne nouvelle :** Les 3 utilisent la **mÃªme configuration** dans ChatAI !

---

## ğŸ“± Interface de Configuration

Dans ChatAI, allez dans **Configuration â†’ IA**, vous verrez maintenant une nouvelle section :

```
ğŸ  SERVEUR LOCAL (Ollama/LM Studio)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ URL du serveur local                         â”‚
â”‚ http://192.168.1.100:11434/v1/chat/completionsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nom du modÃ¨le local                          â”‚
â”‚ llama3.2 ou gpt-oss:20b-cloud                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ Gratuit, privÃ© et illimitÃ© ! 
   Configurez Ollama ou LM Studio sur votre PC.
```

---

## ğŸš€ Option #3 : Ollama Cloud

### Qu'est-ce que c'est ?
ModÃ¨les **puissants hÃ©bergÃ©s** par Ollama (GPT-OSS 120B, DeepSeek 671B).

### Avantages
âœ… **Gratuit** (tier limitÃ©) ou Pro 20$/mois  
âœ… **Aucun GPU requis** sur votre PC  
âœ… **PrivÃ©** - Ollama ne garde pas vos donnÃ©es  
âœ… **Puissant** - Jusqu'Ã  671 milliards de paramÃ¨tres  

### Installation (Quand Ollama est tÃ©lÃ©chargÃ©)

**1. Lancer Ollama et se connecter au cloud**
```bash
# Dans un terminal/PowerShell
ollama signin
# â†’ CrÃ©er un compte sur ollama.com si besoin
```

**2. Tester un modÃ¨le cloud**
```bash
# ModÃ¨les cloud gratuits recommandÃ©s :
ollama run gpt-oss:20b-cloud      # 20B - Ã‰quilibrÃ©
ollama run glm-4.6:cloud          # 4B - Rapide
ollama run kimi-k2:1t-cloud       # 1T - Ã‰norme !

# Si vous avez Pro ($20/mois) :
ollama run gpt-oss:120b-cloud     # 120B - TrÃ¨s puissant
ollama run deepseek-v3.1:671b-cloud # 671B - Le plus puissant
```

**3. Trouver l'IP de votre PC**
```powershell
ipconfig
# â†’ Cherchez "IPv4 Address" (ex: 192.168.1.100)
```

**4. Configurer dans ChatAI**
- Ouvrir ChatAI â†’ Configuration â†’ IA
- Section "SERVEUR LOCAL"
- **URL** : `http://192.168.1.100:11434/v1/chat/completions`
- **ModÃ¨le** : `gpt-oss:20b-cloud`
- Cliquer **SAUVEGARDER**
- Cliquer **TESTER APIs**

**Vous devriez voir :**
```
[3] Local Server: Attempting...
    - URL: http://192.168.1.100:11434/v1/chat/completions
    - Model: gpt-oss:20b-cloud
    - HTTP response: 200
    - Response: Certainly, Michael. I am ready...
[3] Local Server: SUCCESS âœ“
```

---

## ğŸ  Option #5 : Ollama Local

### Qu'est-ce que c'est ?
ModÃ¨les **tournant sur votre PC** - 100% privÃ© et gratuit.

### Avantages
âœ… **100% Gratuit**  
âœ… **Totalement privÃ©** - Rien ne quitte votre rÃ©seau  
âœ… **Offline** - Fonctionne sans internet  
âœ… **Flexible** - Vous choisissez vos modÃ¨les  

### Installation (Quand Ollama est tÃ©lÃ©chargÃ©)

**1. TÃ©lÃ©charger des modÃ¨les locaux**

Pour **CPU uniquement** (pas de GPU) :
```bash
ollama pull llama3.2:3b    # 3B - Bon Ã©quilibre (2 GB RAM)
ollama pull gemma2:2b      # 2B - TrÃ¨s lÃ©ger (1.5 GB RAM)
ollama pull phi3:mini      # 3.8B - Performant (2.3 GB RAM)
```

Pour **avec GPU** (NVIDIA/AMD) :
```bash
ollama pull llama3.1:8b    # 8B - Excellent (5 GB VRAM)
ollama pull mistral:7b     # 7B - TrÃ¨s bon (4 GB VRAM)
ollama pull qwen2.5:14b    # 14B - Puissant (8 GB VRAM)
```

**2. Tester un modÃ¨le**
```bash
ollama run llama3.2:3b
# â†’ Si Ã§a marche, le serveur est prÃªt !
```

**3. Configurer dans ChatAI**
- **URL** : `http://VOTRE_IP:11434/v1/chat/completions`
- **ModÃ¨le** : `llama3.2:3b` (ou celui que vous avez tÃ©lÃ©chargÃ©)
- **SAUVEGARDER** â†’ **TESTER APIs**

### ModÃ¨les recommandÃ©s

| ModÃ¨le | Taille | RAM/VRAM | Vitesse | QualitÃ© | Usage |
|--------|--------|----------|---------|---------|-------|
| **gemma2:2b** | 2B | 1.5 GB | âš¡âš¡âš¡ | â­â­ | CPU faible |
| **llama3.2:3b** | 3B | 2 GB | âš¡âš¡âš¡ | â­â­â­ | CPU moyen |
| **phi3:mini** | 3.8B | 2.3 GB | âš¡âš¡ | â­â­â­ | Ã‰quilibrÃ© |
| **mistral:7b** | 7B | 4 GB | âš¡âš¡ | â­â­â­â­ | GPU entry |
| **llama3.1:8b** | 8B | 5 GB | âš¡âš¡ | â­â­â­â­ | GPU mid |
| **qwen2.5:14b** | 14B | 8 GB | âš¡ | â­â­â­â­â­ | GPU high |

---

## ğŸ® Option #7 : Oobabooga Text Generation WebUI

### Qu'est-ce que c'est ?
Interface web **ultra-configurable** pour LLM, support des gros modÃ¨les (70B+).

### Avantages
âœ… **Maximum de contrÃ´le** (tempÃ©rature, top-p, rÃ©pÃ©tition, etc.)  
âœ… **Gros modÃ¨les** - Jusqu'Ã  70B+ avec quantization  
âœ… **Multi-backends** - llama.cpp, ExLlama, GPTQ, AWQ  
âœ… **Extensions** - RAG, websearch, TTS, etc.  

### Installation (Plus complexe - pour power users)

**1. Installer Oobabooga**
```bash
# Windows - TÃ©lÃ©charger depuis :
# https://github.com/oobabooga/text-generation-webui/releases

# Ou clone + install
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
start_windows.bat  # Windows
# OU
./start_linux.sh   # Linux
# OU
./start_macos.sh   # Mac
```

**2. TÃ©lÃ©charger un modÃ¨le**
- Ouvrir http://localhost:7860
- Onglet "Model" â†’ "Download"
- ModÃ¨les recommandÃ©s :
  - `TheBloke/Llama-2-13B-chat-GGUF` (13B - bon Ã©quilibre)
  - `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` (7B - rapide)
  - `TheBloke/WizardLM-70B-V1.0-GGUF` (70B - trÃ¨s puissant, GPU requis)

**3. Activer l'API OpenAI**
- Onglet "Session"
- Cocher "api" et "openai" dans Extensions
- Ou dÃ©marrer avec :
```bash
start_windows.bat --api --extensions openai
```

**4. VÃ©rifier l'API**
L'API devrait Ãªtre sur : `http://localhost:5000`

**5. Configurer dans ChatAI**
- **URL** : `http://VOTRE_IP:5000/v1/chat/completions`
- **ModÃ¨le** : Le nom du modÃ¨le chargÃ© dans Oobabooga
- **SAUVEGARDER** â†’ **TESTER APIs**

---

## ğŸ”§ RÃ©solution de ProblÃ¨mes

### "Connection refused" ou timeout
**Cause :** Le tÃ©lÃ©phone ne peut pas atteindre le PC

**Solutions :**
1. VÃ©rifier que le PC et le tÃ©lÃ©phone sont sur le **mÃªme WiFi**
2. Trouver l'IP correcte du PC : `ipconfig` (Windows)
3. DÃ©sactiver le **Firewall** temporairement pour tester :
   ```powershell
   # Windows - Autoriser le port 11434 (Ollama)
   netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434
   ```
4. VÃ©rifier que le serveur tourne :
   - Ollama : `ollama list` doit montrer les modÃ¨les
   - Oobabooga : http://localhost:7860 doit s'ouvrir

### "Model not found" ou "HTTP 404"
**Cause :** Le modÃ¨le n'est pas installÃ©

**Solutions :**
- Ollama Cloud : `ollama pull gpt-oss:20b-cloud`
- Ollama Local : `ollama pull llama3.2:3b`
- Oobabooga : TÃ©lÃ©charger via l'interface web

### "HTTP 500" ou erreur serveur
**Cause :** Le modÃ¨le est trop gros pour votre RAM/VRAM

**Solutions :**
- TÃ©lÃ©charger un modÃ¨le plus petit
- Ou utiliser une version quantifiÃ©e (Q4, Q5)
- Ollama : `ollama pull llama3.2:3b-q4_0`

### RÃ©ponse trÃ¨s lente
**Cause :** Le modÃ¨le tourne sur CPU au lieu de GPU

**Solutions :**
- Ollama : VÃ©rifier les logs `ollama serve` pour voir si GPU est dÃ©tectÃ©
- Ou utiliser un modÃ¨le plus petit (2-3B au lieu de 7B)
- Activer GPU : `OLLAMA_GPU=1 ollama serve`

---

## ğŸ“Š Comparaison des 3 Options

| CritÃ¨re | #3 Ollama Cloud | #5 Ollama Local | #7 Oobabooga |
|---------|----------------|----------------|--------------|
| **Setup** | âš¡ 10 min | âš¡ 15 min | âš ï¸ 30 min |
| **CoÃ»t** | Gratuit/20$ | Gratuit | Gratuit |
| **Vitesse** | âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ |
| **QualitÃ©** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **PrivÃ©** | âœ… | âœ…âœ… | âœ…âœ… |
| **Offline** | âŒ | âœ… | âœ… |
| **GPU requis** | âŒ | âŒ | Optionnel |
| **ContrÃ´le** | â­â­ | â­â­â­ | â­â­â­â­â­ |

---

## ğŸ¯ Configuration RecommandÃ©e

**Pour la majoritÃ© des utilisateurs :**
```
URL: http://192.168.1.100:11434/v1/chat/completions
ModÃ¨le: gpt-oss:20b-cloud (si vous avez Ollama Cloud)
     OU llama3.2:3b (si local uniquement)
```

**Pour les power users avec GPU :**
```
URL: http://192.168.1.100:5000/v1/chat/completions
ModÃ¨le: Mistral-7B-Instruct-v0.2 (via Oobabooga)
```

---

## âœ… Prochaines Ã‰tapes

1. **Attendre qu'Ollama finisse de tÃ©lÃ©charger** (~1h restant)
2. **Suivre Option #3** (Ollama Cloud - le plus simple)
3. **Tester KITT** avec "TESTER APIs"
4. **Optionnel** : Installer des modÃ¨les locaux (#5) ou Oobabooga (#7)

---

**KITT sera opÃ©rationnel dÃ¨s qu'Ollama sera installÃ© !** ğŸš€

