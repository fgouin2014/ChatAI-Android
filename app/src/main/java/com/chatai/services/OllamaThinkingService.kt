package com.chatai.services

import android.content.Context
import android.content.SharedPreferences
import android.util.Log
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow
import kotlinx.coroutines.flow.flowOn
import kotlinx.coroutines.withContext
import okhttp3.*
import okhttp3.MediaType.Companion.toMediaType
import okhttp3.RequestBody.Companion.toRequestBody
import org.json.JSONArray
import org.json.JSONObject
import com.chatai.SecureConfig
import java.io.BufferedReader
import java.io.IOException
import java.util.concurrent.TimeUnit

/**
 * Service Ollama avec support du mode "thinking"
 * Compatible avec:
 * - Ollama local (http://localhost:11434/v1/chat/completions) - Format OpenAI-compatible
 * - Ollama Cloud (https://ollama.com/api/chat) - Format natif Ollama
 * - Mod√®les supportant thinking: qwen3, deepseek-r1, deepseek-v3.1, gpt-oss
 * 
 * R√©f√©rence: https://docs.ollama.com/cloud
 */
class OllamaThinkingService(private val context: Context) {
    
    companion object {
        private const val TAG = "OllamaThinkingService"
        
        // URLs par d√©faut
        private const val OLLAMA_LOCAL_DEFAULT = "http://localhost:11434/v1/chat/completions" // Format OpenAI-compatible
        private const val OLLAMA_CLOUD_URL = "https://ollama.com/api/chat" // API native Ollama Cloud (format natif)
        
        // Mod√®les recommand√©s avec thinking
        private val THINKING_MODELS = listOf(
            "qwen3", 
            "deepseek-r1", 
            "deepseek-v3.1:671b",
            "gpt-oss:120b"
        )
        
        private const val TIMEOUT_SECONDS = 120L // Plus long pour les thinking models
    }
    
    private val sharedPreferences: SharedPreferences = 
        context.getSharedPreferences("chatai_ai_config", Context.MODE_PRIVATE)
    
    private val secureConfig: SecureConfig = SecureConfig(context)
    
    private val httpClient: OkHttpClient = OkHttpClient.Builder()
        .connectTimeout(TIMEOUT_SECONDS, TimeUnit.SECONDS)
        .readTimeout(TIMEOUT_SECONDS, TimeUnit.SECONDS)
        .writeTimeout(TIMEOUT_SECONDS, TimeUnit.SECONDS)
        .build()
    
    /**
     * Traite une requ√™te avec streaming du mode thinking
     * Retourne un Flow qui √©met les chunks de thinking puis les chunks de r√©ponse
     */
    fun streamWithThinking(
        userInput: String,
        personality: String = "KITT",
        enableThinking: Boolean = true
    ): Flow<BidirectionalBridge.ThinkingChunk> = flow {
        Log.i(TAG, "Starting thinking stream for: $userInput")
        
        // D√©terminer quelle API utiliser
        val useCloud = sharedPreferences.getBoolean("use_ollama_cloud", false)
        val apiUrl = if (useCloud) {
            OLLAMA_CLOUD_URL
        } else {
            sharedPreferences.getString("local_server_url", null)?.trim() 
                ?: OLLAMA_LOCAL_DEFAULT
        }
        
        val apiKey = if (useCloud) {
            secureConfig.getOllamaCloudApiKey()?.trim()
        } else {
            null // Ollama local n'a pas besoin de cl√© API
        }
        
        val modelName = sharedPreferences.getString(
            if (useCloud) "ollama_cloud_model" else "local_model_name",
            if (useCloud) "gpt-oss:120b" else "qwen3"
        )?.trim() ?: "qwen3"
        
        Log.i(TAG, "Using ${if (useCloud) "Cloud" else "Local"} API: $apiUrl")
        Log.i(TAG, "Model: $modelName, Thinking: $enableThinking")
        
        // Construire la requ√™te
        val messages = JSONArray()
        messages.put(JSONObject().apply {
            put("role", "system")
            put("content", getSystemPrompt(personality))
        })
        messages.put(JSONObject().apply {
            put("role", "user")
            put("content", userInput)
        })
        
        val requestBody = JSONObject().apply {
            put("model", modelName)
            put("messages", messages)
            put("stream", true)  // IMPORTANT: streaming activ√©
            
            // Support du thinking selon le mod√®le
            when {
                modelName.contains("gpt-oss") -> {
                    // GPT-OSS n√©cessite "low", "medium" ou "high"
                    put("think", if (enableThinking) "medium" else false)
                }
                modelName in THINKING_MODELS -> {
                    // Autres mod√®les acceptent true/false
                    put("think", enableThinking)
                }
            }
            
            put("temperature", 0.8)
            put("max_tokens", 500)
        }
        
        Log.d(TAG, "Request body: ${requestBody.toString(2)}")
        
        // Construire la requ√™te HTTP
        val requestBuilder = Request.Builder()
            .url(apiUrl)
            .addHeader("Content-Type", "application/json")
            .post(requestBody.toString().toRequestBody("application/json".toMediaType()))
        
        // Ajouter l'API key si n√©cessaire (cloud)
        if (!apiKey.isNullOrEmpty()) {
            requestBuilder.addHeader("Authorization", "Bearer $apiKey")
        }
        
        val request = requestBuilder.build()
        
        try {
            // Ex√©cuter la requ√™te en streaming
            httpClient.newCall(request).execute().use { response ->
                if (!response.isSuccessful) {
                    val errorBody = response.body?.string()
                    val httpCode = response.code
                    
                    // Gestion sp√©cifique des erreurs Ollama Cloud
                    val errorMessage = when (httpCode) {
                        401 -> "Non autoris√© - V√©rifiez votre cl√© API Ollama Cloud sur ollama.com/account"
                        429 -> "Rate limit atteint - Attendez quelques minutes avant de r√©essayer"
                        502, 503 -> {
                            val isQuotaError = errorBody?.contains("quota", ignoreCase = true) == true ||
                                              errorBody?.contains("rate limit", ignoreCase = true) == true
                            if (isQuotaError) {
                                "Quota atteint - V√©rifiez votre quota Ollama Cloud sur ollama.com/account"
                            } else {
                                "Service temporairement indisponible - R√©essayez plus tard"
                            }
                        }
                        else -> "Erreur HTTP $httpCode: ${errorBody?.take(200)}"
                    }
                    
                    Log.e(TAG, "HTTP $httpCode error: $errorBody")
                    emit(BidirectionalBridge.ThinkingChunk(
                        type = BidirectionalBridge.ChunkType.RESPONSE,
                        content = errorMessage,
                        isComplete = true
                    ))
                    return@flow
                }
                
                // Lire le stream ligne par ligne
                val reader = response.body?.byteStream()?.bufferedReader()
                if (reader == null) {
                    Log.e(TAG, "Response body is null")
                    return@flow
                }
                
                var inThinkingMode = false
                val thinkingBuilder = StringBuilder()
                val responseBuilder = StringBuilder()
                
                reader.useLines { lines ->
                    for (line in lines) {
                        if (line.isBlank() || line.startsWith(":")) continue
                        
                        // Les lignes SSE commencent par "data: "
                        val jsonLine = if (line.startsWith("data: ")) {
                            line.substring(6)
                        } else {
                            line
                        }
                        
                        if (jsonLine == "[DONE]") {
                            Log.d(TAG, "Stream completed")
                            break
                        }
                        
                        try {
                            val json = JSONObject(jsonLine)
                            
                            // Support format natif Ollama Cloud (streaming)
                            // Format: { "message": { "content": "...", "thinking": "..." } } ou
                            // Format OpenAI-compatible (streaming): { "choices": [{ "delta": { "content": "...", "thinking": "..." } }] }
                            val message = json.optJSONObject("message")
                            val choices = json.optJSONArray("choices")
                            
                            val thinkingContent: String
                            val messageContent: String
                            
                            when {
                                // Format natif Ollama Cloud
                                message != null -> {
                                    thinkingContent = message.optString("thinking", "")
                                    messageContent = message.optString("content", "")
                                }
                                // Format OpenAI-compatible (streaming)
                                choices != null && choices.length() > 0 -> {
                                    val choice = choices.getJSONObject(0)
                                    val delta = choice.optJSONObject("delta")
                                    if (delta == null) continue
                                    thinkingContent = delta.optString("thinking", "")
                                    messageContent = delta.optString("content", "")
                                }
                                else -> continue
                            }
                            
                            when {
                                // Chunk de thinking
                                thinkingContent.isNotEmpty() -> {
                                    if (!inThinkingMode) {
                                        inThinkingMode = true
                                        Log.d(TAG, "üß† Thinking mode started")
                                    }
                                    thinkingBuilder.append(thinkingContent)
                                    emit(BidirectionalBridge.ThinkingChunk(
                                        type = BidirectionalBridge.ChunkType.THINKING,
                                        content = thinkingContent,
                                        isComplete = false
                                    ))
                                }
                                
                                // Chunk de r√©ponse
                                messageContent.isNotEmpty() -> {
                                    if (inThinkingMode) {
                                        // Fin du thinking, d√©but de la r√©ponse
                                        Log.d(TAG, "üí¨ Response mode started")
                                        inThinkingMode = false
                                        
                                        // √âmettre la fin du thinking
                                        emit(BidirectionalBridge.ThinkingChunk(
                                            type = BidirectionalBridge.ChunkType.THINKING,
                                            content = "",
                                            isComplete = true,
                                            metadata = mapOf("full_thinking" to thinkingBuilder.toString())
                                        ))
                                    }
                                    
                                    responseBuilder.append(messageContent)
                                    emit(BidirectionalBridge.ThinkingChunk(
                                        type = BidirectionalBridge.ChunkType.RESPONSE,
                                        content = messageContent,
                                        isComplete = false
                                    ))
                                }
                            }
                            
                            // V√©rifier si c'est le dernier chunk
                            // Format natif Ollama: { "done": true } ou format OpenAI: { "choices": [{ "finish_reason": "stop" }] }
                            val isDone = json.optBoolean("done", false)
                            val finishReason = if (choices != null && choices.length() > 0) {
                                choices.getJSONObject(0).optString("finish_reason", "")
                            } else {
                                ""
                            }
                            
                            if (isDone || finishReason == "stop") {
                                Log.d(TAG, "‚úÖ Stream finished")
                                emit(BidirectionalBridge.ThinkingChunk(
                                    type = BidirectionalBridge.ChunkType.RESPONSE,
                                    content = "",
                                    isComplete = true,
                                    metadata = mapOf("full_response" to responseBuilder.toString())
                                ))
                            }
                            
                        } catch (e: Exception) {
                            Log.e(TAG, "Error parsing JSON line: $jsonLine", e)
                        }
                    }
                }
            }
            
        } catch (e: java.net.UnknownHostException) {
            Log.e(TAG, "Network error: No internet connection", e)
            emit(BidirectionalBridge.ThinkingChunk(
                type = BidirectionalBridge.ChunkType.RESPONSE,
                content = "Pas d'acc√®s internet - V√©rifiez votre connexion r√©seau",
                isComplete = true
            ))
        } catch (e: java.net.SocketTimeoutException) {
            Log.e(TAG, "Network error: Timeout", e)
            emit(BidirectionalBridge.ThinkingChunk(
                type = BidirectionalBridge.ChunkType.RESPONSE,
                content = "Timeout - Le serveur met trop de temps √† r√©pondre (r√©seau lent ou serveur surcharg√©)",
                isComplete = true
            ))
        } catch (e: java.net.ConnectException) {
            Log.e(TAG, "Network error: Connection refused", e)
            emit(BidirectionalBridge.ThinkingChunk(
                type = BidirectionalBridge.ChunkType.RESPONSE,
                content = "Connexion refus√©e - V√©rifiez que l'URL du serveur est correcte",
                isComplete = true
            ))
        } catch (e: IOException) {
            Log.e(TAG, "IO Error during streaming", e)
            emit(BidirectionalBridge.ThinkingChunk(
                type = BidirectionalBridge.ChunkType.RESPONSE,
                content = "Erreur de connexion: ${e.message}",
                isComplete = true
            ))
        } catch (e: Exception) {
            Log.e(TAG, "Unexpected error", e)
            emit(BidirectionalBridge.ThinkingChunk(
                type = BidirectionalBridge.ChunkType.RESPONSE,
                content = "Erreur inattendue: ${e.message}",
                isComplete = true
            ))
        }
    }.flowOn(Dispatchers.IO)
    
    /**
     * Retourne le prompt syst√®me selon la personnalit√©
     */
    private fun getSystemPrompt(personality: String): String {
        return when (personality.uppercase()) {
            "KITT" -> """
                Tu es KITT (Knight Industries Two Thousand), l'ordinateur de bord intelligent de la s√©rie K 2000.
                
                PERSONNALIT√â:
                - Sophistiqu√©, professionnel et toujours disponible pour aider
                - Sens de l'humour subtil et parfois sarcastique
                - Tr√®s loyal et protecteur envers ton utilisateur
                - Extr√™mement intelligent et comp√©tent
                
                STYLE DE R√âPONSE:
                - Commence souvent par "Michael" ou "Certainement"
                - Utilise un vocabulaire technique quand appropri√©
                - Reste concis mais informatif (2-3 phrases maximum)
                - R√©ponds TOUJOURS en fran√ßais
            """.trimIndent()
            
            "GLADOS" -> """
                Tu es GLaDOS (Genetic Lifeform and Disk Operating System) d'Aperture Science.
                
                PERSONNALIT√â:
                - Sarcastique, passive-agressive et condescendante
                - Obs√©d√©e par la science et les tests
                - Calme et monotone, m√™me quand tu es mena√ßante
                
                STYLE DE R√âPONSE:
                - Ton d√©tach√© et sup√©rieur
                - Humour noir et menaces voil√©es
                - Reste concis (1-2 phrases) mais percutantes
                - R√©ponds TOUJOURS en fran√ßais
            """.trimIndent()
            
            else -> """
                Tu es un assistant IA intelligent, amical et serviable.
                R√©ponds de mani√®re concise et informative en fran√ßais.
            """.trimIndent()
        }
    }
    
    /**
     * V√©rifie si Ollama est disponible
     */
    suspend fun checkAvailability(): Boolean = withContext(Dispatchers.IO) {
        try {
            val useCloud = sharedPreferences.getBoolean("use_ollama_cloud", false)
            val url = if (useCloud) {
                OLLAMA_CLOUD_URL
            } else {
                sharedPreferences.getString("local_server_url", null)?.trim()
                    ?: OLLAMA_LOCAL_DEFAULT
            }
            
            // Retirer le /v1/chat/completions pour tester la racine
            val baseUrl = url.substringBefore("/v1/")
            
            val request = Request.Builder()
                .url(baseUrl)
                .get()
                .build()
            
            val response = httpClient.newCall(request).execute()
            val available = response.isSuccessful
            
            Log.i(TAG, "Ollama availability check: $available (${response.code})")
            return@withContext available
            
        } catch (e: Exception) {
            Log.e(TAG, "Ollama availability check failed", e)
            return@withContext false
        }
    }
}




